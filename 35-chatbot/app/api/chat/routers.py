# https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py
import time
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import numpy as np
import random
from typing import Annotated

from ...core import Session
from ...models import UserDB, ModelDB, ConversationDB

from ...dependencies import (
    verify_access_token,
    oauth2_scheme,
)


DEFAULT_MODEL = "gpt4o"


router = APIRouter()
# 可以使用它来声明*路径操作*。
# 使用方式与 FastAPI 类相同：
# 你可以将 APIRouter 视为一个「迷你 FastAPI」类。
# 所有相同的选项都得到支持。
# 所有相同的 parameters、responses、dependencies、tags 等等。


# 创建数据库会话
session = Session()


# 与声明查询参数一样，包含默认值的模型属性是可选的，否则就是必选的。默认值为 None 的模型属性也是可选的。
class ChatRequest(BaseModel):
    model: str | None = Field(
        None,
        description="The model used for generating the response",
        examples=["gpt4o", "gpt4"],
    )
    messages: list[dict[str, str]] = Field(
        None,
        description="List of dictionaries containing the input text and the corresponding user id",
        examples=[[{"role": "user", "content": "你是谁?"}]],
    )
    max_tokens: int = Field(
        1024, ge=1, le=2048, description="Maximum number of new tokens to generate"
    )
    n: int = Field(
        1,
        ge=1,
        le=10,
        description="Number of completions to generate for each prompt",
    )
    temperature: float = Field(
        0.8,
        ge=0.1,
        le=2.0,
        description="Sampling temperature (lower temperature results in less random completions",
    )
    top_p: float = Field(
        0.8,
        ge=0.0,
        le=1.0,
        description="Nucleus sampling top-p (top-p sampling chooses from the smallest possible set of tokens whose cumulative probability mass exceeds the probability top_p)",
    )
    top_k: int = Field(
        50,
        ge=0,
        le=100,
        description="Top-k sampling chooses from the top k tokens with highest probability",
    )
    stream: bool = Field(
        False,
        description="Whether to stream the output or wait for the whole response before returning it",
    )
    conversation_id: int | None = Field(
        None,
        description="The id of the conversation",
        examples=[
            123456,
        ],
    )


# -------------------- 非流式响应模型 --------------------#
class ChatCompletionMessage(BaseModel):
    content: str | None = Field(
        None,
        description="The input text of the user or assistant",
        examples=["你是谁?"],
    )
    # 允许添加额外字段
    reference: list[str] | None = Field(
        None,
        description="The reference text(s) used for generating the response",
        examples=[["book1", "book2"]],
    )
    role: str = Field(
        None,
        description="The role of the user or assistant",
        examples=["system", "user", "assistant"],
    )
    refusal: bool = Field(
        False,
        description="Whether the user or assistant refused to provide a response",
        examples=[False, True],
    )
    function_call: str | None = Field(
        None,
        description="The function call that the user or assistant made",
        examples=["ask_name", "ask_age", "ask_location"],
    )
    tool_calls: str | None = Field(
        None,
        description="The tool calls that the user or assistant made",
        examples=["weather", "calendar", "news"],
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


class ChatCompletionChoice(BaseModel):
    index: int = Field(
        None,
        description="The index of the choice",
        examples=[0, 1, 2],
    )
    finish_reason: str | None = Field(
        None,
        description="The reason for finishing the conversation",
        examples=[None, "stop"],
    )
    logprobs: list[float] | None = Field(
        None,
        description="The log probabilities of the choices",
        examples=[-1.3862943611198906, -1.3862943611198906, -1.3862943611198906],
    )
    message: ChatCompletionMessage | None = Field(
        None,
        description="The message generated by the model",
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


class CompletionUsage(BaseModel):
    prompt_tokens: int = Field(
        0,
        description="The number of tokens in the prompt",
        examples=[10],
    )
    completion_tokens: int = Field(
        0,
        description="The number of tokens in the completion",
        examples=[10],
    )
    total_tokens: int = Field(
        0,
        description="The total number of tokens generated",
        examples=[10],
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


class ChatCompletion(BaseModel):
    id: str | int | None = Field(
        None,
        description="The id of the conversation",
        examples=[123456, "abc123"],
    )
    choices: list[ChatCompletionChoice] = Field(
        [],
        description="The choices generated by the model",
    )
    created: int | float | None = Field(
        None,
        description="The timestamp when the conversation was created",
    )
    model: str | None = Field(
        None,
        description="The model used for generating the response",
        examples=["gpt4o", "gpt4"],
    )
    object: str = Field(
        "chat.completion",
        description="The object of the conversation",
        examples=["chat.completion"],
    )
    service_tier: str | None = Field(
        None,
        description="The service tier of the conversation",
        examples=["basic", "premium"],
    )
    system_fingerprint: str | None = Field(
        None,
        description="The system fingerprint of the conversation",
        examples=["1234567890abcdef"],
    )
    usage: CompletionUsage = Field(
        CompletionUsage(),
        description="The usage of the completion",
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


# -------------------- 非流式响应模型 --------------------#


# -------------------- 流式响应模型 --------------------#
class ChoiceDelta(ChatCompletionMessage): ...


class ChatCompletionChunkChoice(BaseModel):
    index: int = Field(
        None,
        description="The index of the choice",
        examples=[0, 1, 2],
    )
    finish_reason: str | None = Field(
        None,
        description="The reason for finishing the conversation",
        examples=[None, "stop"],
    )
    logprobs: list[float] | None = Field(
        None,
        description="The log probabilities of the choices",
        examples=[-1.3862943611198906, -1.3862943611198906, -1.3862943611198906],
    )
    delta: ChoiceDelta | None = Field(
        None,
        description="The message generated by the model",
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


class ChatCompletionChunk(BaseModel):
    id: str | int | None = Field(
        None,
        description="The id of the conversation",
        examples=[123456, "abc123"],
    )
    choices: list[ChatCompletionChunkChoice] = Field(
        [],
        description="The choices generated by the model",
    )
    created: int | float | None = Field(
        None,
        description="The timestamp when the conversation was created",
    )
    model: str | None = Field(
        None,
        description="The model used for generating the response",
        examples=["gpt4o", "gpt4"],
    )
    object: str = Field(
        "chat.completion.chunk",
        description="The object of the conversation",
        examples=["chat.completion.chunk"],
    )
    service_tier: str | None = Field(
        None,
        description="The service tier of the conversation",
        examples=["basic", "premium"],
    )
    system_fingerprint: str | None = Field(
        None,
        description="The system fingerprint of the conversation",
        examples=["1234567890abcdef"],
    )
    usage: CompletionUsage = Field(
        None,
        description="The usage of the completion",
    )

    def __repr__(self) -> str:
        return self.model_dump_json()


# 将请求体作为 JSON 读取
# 在函数内部，你可以直接访问模型对象的所有属性
# http://127.0.0.1:8000/docs
@router.post("/v1/chat/completions", response_model=ChatCompletion)
async def chat(request: ChatRequest, token: Annotated[str, Depends(oauth2_scheme)]):
    user_id = int(verify_access_token(token))
    print(user_id)

    # 验证用户和模型
    user: UserDB = session.query(UserDB).get(user_id)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid user")
    model_name = request.model or DEFAULT_MODEL
    model = session.query(ModelDB).filter(ModelDB.model_name == model_name).first()
    if not model:
        model = ModelDB(model_name=model_name)
        session.add(model)
        session.commit()

    print(request)
    if not request.messages or len(request.messages) == 0:
        raise HTTPException(status_code=400, detail="No messages provided")

    role = request.messages[-1].get("role", "")
    if role not in ["user", "assistant"]:
        raise HTTPException(status_code=400, detail="Invalid role")

    content = request.messages[-1].get("content", "")
    if not content:
        raise HTTPException(status_code=400, detail="content is empty")
    content_len = len(content)

    number = str(np.random.randint(0, 100, 10))
    print(f"number: {number}")

    id = random.getrandbits(64)

    # 流式响应
    if request.stream:

        async def generate():
            for i, n in enumerate(number):
                time.sleep(0.2)
                response = ChatCompletionChunk(
                    id=id,
                    choices=[
                        ChatCompletionChunkChoice(
                            index=0,
                            finish_reason=None,
                            delta=ChoiceDelta(
                                content=n,
                                role="assistant",
                            ),
                        )
                    ],
                    created=time.time(),
                    usage=CompletionUsage(
                        prompt_tokens=content_len,
                        completion_tokens=i + 1,
                        total_tokens=content_len + i + 1,
                    ),
                )
                print(response)
                # openai api returns \n\n as a delimiter for messages
                yield f"data: {response.model_dump_json()}\n\n"

            response = ChatCompletionChunk(
                id=id,
                choices=[
                    ChatCompletionChunkChoice(
                        index=0,
                        finish_reason="stop",
                        delta=ChoiceDelta(),
                    )
                ],
                created=time.time(),
                usage=CompletionUsage(
                    prompt_tokens=content_len,
                    completion_tokens=len(number),
                    total_tokens=content_len + len(number),
                ),
            )
            print(response)
            # openai api returns \n\n as a delimiter for messages
            yield f"data: {response.model_dump_json()}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(generate())

    # 非流式响应
    response = ChatCompletion(
        id=id,
        choices=[
            ChatCompletionChoice(
                index=0,
                finish_reason="stop",
                message=ChatCompletionMessage(
                    content=number,
                    role="assistant",
                ),
            ),
        ],
        created=time.time(),
        usage=CompletionUsage(
            prompt_tokens=content_len,
            completion_tokens=len(number),
            total_tokens=content_len + len(number),
        ),
    )
    print(response)

    # 保存到数据库
    messages = request.messages
    input_tokens = sum(len(message["content"]) for message in messages)
    output_tokens = len(number)
    messages.append({"role": "assistant", "content": number})
    conversation_id = request.conversation_id
    conversation: ConversationDB = session.query(ConversationDB).get(conversation_id)
    if conversation:
        conversation.messages = messages
        conversation.model = model
        conversation.input_tokens = input_tokens
        conversation.output_tokens = output_tokens
        session.commit()
    else:
        conversation = ConversationDB(
            messages=messages,
            user=user,
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
        )
        session.add(conversation)
        session.commit()

    return response


# 与声明查询参数一样，包含默认值的模型属性是可选的，否则就是必选的。默认值为 None 的模型属性也是可选的。
class Messages(BaseModel):
    id: int = Field(
        None,
        description="The id of the conversation",
    )
    user_id: int | None = Field(
        None,
        description="The id of the user",
    )
    model: str | None = Field(
        None,
        description="The name of the model",
    )
    title: str | None = Field(
        None,
        description="The title of the conversation",
    )
    messages: list[dict[str, str]] | None = Field(
        None,
        description="List of dictionaries containing the input text and the corresponding user id",
        examples=[[{"role": "user", "content": "你是谁?"}]],
    )
    desc: str | None = Field(
        None,
        description="The description of the conversation",
    )


@router.get("/history", response_model=list[Messages])
async def history(
    token: Annotated[str, Depends(oauth2_scheme)],
    skip: int = Query(default=0, ge=0),
    limit: int = Query(default=10, ge=1, le=100),
):
    user_id = int(verify_access_token(token))
    print(user_id)

    # 验证用户和模型
    user: UserDB = session.query(UserDB).get(user_id)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid user")

    # conversations: list[ConversationDB] = user.conversations
    conversations: list[ConversationDB] = (
        session.query(ConversationDB)
        .filter(ConversationDB.user_id == user_id)
        .offset(skip)
        .limit(limit)
        .all()
    )

    if not conversations:
        return []

    responses = []
    for conversation in conversations:
        responses.append(
            Messages(
                id=conversation.id,
                user_id=user_id,
                title=conversation.title,
                messages=conversation.messages,
                desc=conversation.desc,
                model=conversation.model.model_name,
            )
        )

    return responses
